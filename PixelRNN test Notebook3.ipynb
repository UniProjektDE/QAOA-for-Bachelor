{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zazabla\\AppData\\Local\\Temp\\ipykernel_14760\\1935846830.py:462: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  x = torch.tensor(X, dtype=torch.float32)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | lstm    | LSTM    | 288   \n",
      "1 | linear  | Linear  | 5     \n",
      "2 | sigmoid | Sigmoid | 0     \n",
      "------------------------------------\n",
      "293       Trainable params\n",
      "0         Non-trainable params\n",
      "293       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "c:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:293: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s] tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1.]]])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'testsdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 508\u001b[0m\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfertig!!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m#create_data_mit_abweichung_pool([0.5, 1, 1.5, 2, 2.5, 3, 3.5], [0.5, 1, 1.5, 2, 2.5, 3, 3.5], beta_grenzen=[1.5,2.5], gamma_grenzen=[1.5,2.5], showDia=True, savePath=\"LSTM Model2 parameter random pool_1\", random_sets=10, max_iter=300, rndMitAnaly=True)\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m#writefile_QAOA(epoch=400, layers=1, shots=1024)\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;66;03m#writefile_QAOA_training_plus_NN(epoch=600, layers=4, shots=960, add_NN_samples=64, max_iter=80, fileprefix=\"896vs64\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m#writefile_QAOA_training_plus_NN(epoch=600, layers=4, shots=64, add_NN_samples=980, max_iter=80, fileprefix=\"64vs980\")\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;66;03m#writefile_QAOA_training_plus_NN(epoch=600, layers=4, shots=32, add_NN_samples=992, max_iter=80, fileprefix=\"32vs992\")\u001b[39;00m\n\u001b[1;32m--> 508\u001b[0m \u001b[43mwritefile_QAOA_training_plus_NN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_NN_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1008\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfileprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m16vs1008\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;66;03m#writefile_QAOA_training_plus_NN(epoch=600, layers=4, shots=8, add_NN_samples=1016, max_iter=80, fileprefix=\"8vs1016\")\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \n\u001b[0;32m    511\u001b[0m \u001b[38;5;66;03m#writefile(n = 100, epoch=200)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 483\u001b[0m, in \u001b[0;36mwritefile_QAOA_training_plus_NN\u001b[1;34m(n, epoch, layers, shots, add_NN_samples, max_iter, fileprefix)\u001b[0m\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NNexpec, shadow\n\u001b[0;32m    482\u001b[0m fileString \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 483\u001b[0m res1, shadow1, expec1 \u001b[38;5;241m=\u001b[39m \u001b[43mrun_qaoa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallByExp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munterfunktion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# QAOA Simualtion gemischt mit NN\u001b[39;00m\n\u001b[0;32m    484\u001b[0m res2, shadow2, expec2 \u001b[38;5;241m=\u001b[39m run_qaoa(G, layers\u001b[38;5;241m=\u001b[39mlayers, shots\u001b[38;5;241m=\u001b[39mshots, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_iter\u001b[38;5;241m=\u001b[39mmax_iter) \u001b[38;5;66;03m# QAOA Simualtion ohne NN\u001b[39;00m\n\u001b[0;32m    485\u001b[0m res3, shadow3, expec3 \u001b[38;5;241m=\u001b[39m run_qaoa(G, layers\u001b[38;5;241m=\u001b[39mlayers, shots\u001b[38;5;241m=\u001b[39mshots \u001b[38;5;241m+\u001b[39m add_NN_samples, max_iter\u001b[38;5;241m=\u001b[39mmax_iter) \u001b[38;5;66;03m# QAOA Simualtion ohne NN, aber mit extra Samples\u001b[39;00m\n",
      "File \u001b[1;32md:\\Uni\\.Aktuelle Dinge\\Bachelor\\Projekt Notebooks\\Arbeitsbibliothek\\qiskit_qaoa.py:178\u001b[0m, in \u001b[0;36mrun_qaoa\u001b[1;34m(graph, layers, shots, callByExp, seed, max_iter)\u001b[0m\n\u001b[0;32m    175\u001b[0m     initial_parameters\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# Minimaze Hamiltonian\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpectation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minitial_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCOBYLA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m                  \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m gw \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mto_numpy_array(graph)\n\u001b[0;32m    184\u001b[0m shadowList_ \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(shadowList) \u001b[38;5;66;03m# muss gemacht werden, da sonst Referenz wiedergeben werden kann\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:716\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    714\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcobyla\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 716\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_cobyla\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslsqp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    719\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_slsqp(fun, x0, args, jac, bounds,\n\u001b[0;32m    720\u001b[0m                           constraints, callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\scipy\\optimize\\_cobyla_py.py:35\u001b[0m, in \u001b[0;36msynchronized.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _module_lock:\n\u001b[1;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\scipy\\optimize\\_cobyla_py.py:278\u001b[0m, in \u001b[0;36m_minimize_cobyla\u001b[1;34m(fun, x0, args, constraints, rhobeg, tol, maxiter, disp, catol, callback, bounds, **unknown_options)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_jac\u001b[39m(x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 278\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_jac\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalcfc\u001b[39m(x, con):\n\u001b[0;32m    281\u001b[0m     f \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun(x)\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:383\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    379\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 383\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32md:\\Uni\\.Aktuelle Dinge\\Bachelor\\Projekt Notebooks\\Arbeitsbibliothek\\qiskit_qaoa.py:155\u001b[0m, in \u001b[0;36mget_expectation.<locals>.execute_circ\u001b[1;34m(theta)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callByExc\u001b[38;5;241m!=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m running_index\n\u001b[1;32m--> 155\u001b[0m     expec, shadow \u001b[38;5;241m=\u001b[39m \u001b[43mcallByExc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshadow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m     running_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    157\u001b[0m s \u001b[38;5;241m=\u001b[39m ShadowShot(shadow, theta_, dim \u001b[38;5;241m=\u001b[39m p)\n",
      "Cell \u001b[1;32mIn[1], line 470\u001b[0m, in \u001b[0;36mwritefile_QAOA_training_plus_NN.<locals>.unterfunktion\u001b[1;34m(expec, G, theta, result, shadow, running_index)\u001b[0m\n\u001b[0;32m    468\u001b[0m model \u001b[38;5;241m=\u001b[39m BitGeneratorModel(num_param, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m) \u001b[38;5;66;03m# MyPixelRNN(num_param, hidden_size)\u001b[39;00m\n\u001b[0;32m    469\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39mmax_iter, accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 470\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m gw \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mto_numpy_array(G)\n\u001b[0;32m    474\u001b[0m array \u001b[38;5;241m=\u001b[39m generate_bitstrings(model, x, N, bitlength)\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    576\u001b[0m     ckpt_path,\n\u001b[0;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    579\u001b[0m )\n\u001b[1;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 989\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    994\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1035\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1035\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:359\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:136\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:240\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    242\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:187\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[1;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m         closure()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:265\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[1;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 157\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    160\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1291\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1254\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1257\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1258\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[0;32m   1261\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1289\u001b[0m \n\u001b[0;32m   1290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1291\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:151\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:230\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[1;32m--> 230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:117\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[1;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\torch\\optim\\adam.py:146\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 146\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    149\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:104\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[1;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     93\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     94\u001b[0m     optimizer: Optimizer,\n\u001b[0;32m     95\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m     96\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:126\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[1;32m--> 126\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:315\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m    314\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_result_cls\u001b[38;5;241m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[38;5;241m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:382\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 94\u001b[0m, in \u001b[0;36mBitGeneratorModel.training_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     92\u001b[0m prev_bits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((batch_size, \u001b[38;5;241m0\u001b[39m), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sequence_length):\n\u001b[1;32m---> 94\u001b[0m     prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_bits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     y_t \u001b[38;5;241m=\u001b[39m y[:, t:t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Ziel-Bit für den aktuellen Zeitschritt\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m#### Test\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zazabla\\anaconda3\\envs\\qiskit2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 73\u001b[0m, in \u001b[0;36mBitGeneratorModel.forward\u001b[1;34m(self, x, prev_bits)\u001b[0m\n\u001b[0;32m     71\u001b[0m     combined_input \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(combined_input)\n\u001b[1;32m---> 73\u001b[0m     \u001b[43mtestsdf\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Berechnen der Wahrscheinlichkeit für den nächsten Bit\u001b[39;00m\n\u001b[0;32m     75\u001b[0m output, (hn, cn) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(combined_input)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'testsdf' is not defined"
     ]
    }
   ],
   "source": [
    "from Arbeitsbibliothek.myshadow import ShadowShot, ShadowCollection\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "#from torchviz import make_dot\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from Arbeitsbibliothek.myshadow import ShadowShot, ShadowCollection\n",
    "import random\n",
    "from Arbeitsbibliothek.mytoolbox import make_dia\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pprint\n",
    "from Arbeitsbibliothek.qiskit_qaoa import run_qaoa, run_single_qaoa_cir\n",
    "import networkx as nx\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "def cluster_string(ersatz_string=\"\"):\n",
    "    try:\n",
    "        name = sys.argv[2]\n",
    "    except IndexError:\n",
    "        name = ersatz_string\n",
    "    return name\n",
    "\n",
    "def verbinde_zu_array(a, b):\n",
    "    # Prüfe, ob 'a' oder 'b' ein NumPy-Array ist und wandle entsprechend um\n",
    "    if isinstance(a, list):\n",
    "        a = np.array(a)\n",
    "    if isinstance(b, list):\n",
    "        b = np.array(b)\n",
    "    \n",
    "    return np.concatenate((a, b))\n",
    "\n",
    "#os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "lauf_variable = 0\n",
    "loss_index = []\n",
    "loss_curve = []\n",
    "def log_loss(loss):\n",
    "    global lauf_variable\n",
    "    loss_curve.append(loss.item())\n",
    "    loss_index.append(lauf_variable)\n",
    "    lauf_variable += 1\n",
    "\n",
    "class BitGeneratorModel(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_size = 2):\n",
    "        super().__init__()\n",
    "        output_size = 1\n",
    "        self.lstm = nn.LSTM(input_dim + hidden_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.lr = 0.01\n",
    "\n",
    "    def forward(self, x, prev_bits):\n",
    "        \n",
    "        # Kombinieren des Inputs und der bisher generierten Bits\n",
    "        '''\n",
    "        tensor1_expanded = x.unsqueeze(1).expand(-1, prev_bits.size(1), -1)\n",
    "        tensor2_expanded = prev_bits.unsqueeze(2)\n",
    "        combined_input = torch.cat((tensor1_expanded, tensor2_expanded), dim=2)\n",
    "        print(\"jdjd\", combined_input.size())\n",
    "        '''\n",
    "        if prev_bits.size(1) > 0:  # Überprüfen, ob prev_bits eine positive Sequenzlänge hat\n",
    "            tensor1_expanded = x.unsqueeze(1).expand(-1, prev_bits.size(1), -1)\n",
    "            tensor2_expanded = prev_bits.unsqueeze(2)\n",
    "            combined_input = torch.cat((tensor1_expanded, tensor2_expanded), dim=2)\n",
    "        else:\n",
    "            # Erstellen eines Dummy-Tensors mit Sequenzlänge 1, um das LSTM zu befriedigen\n",
    "            #print(x)\n",
    "            combined_input = x.unsqueeze(1)\n",
    "            #print(combined_input)\n",
    "            #testsdf\n",
    "        # Berechnen der Wahrscheinlichkeit für den nächsten Bit\n",
    "        output, (hn, cn) = self.lstm(combined_input)\n",
    "        output = output[:,-1,:]\n",
    "\n",
    "        prob = self.linear(output)  # Nehmen Sie nur den letzten Ausgang für jede Sequenz\n",
    "        prob = self.sigmoid(prob)\n",
    "        #prob = prob.squeeze(-1)\n",
    "        return prob\n",
    "        \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        batch_size, sequence_length = y.shape\n",
    "        #loss_fn = nn.BCELoss(reduction='mean')\n",
    "        loss_fn = torch.nn.SmoothL1Loss(reduction='mean')\n",
    "        #loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "        #loss_fn = torch.nn.PoissonNLLLoss(reduction='mean', full=True)\n",
    "        loss = 0\n",
    "        prev_bits = torch.zeros((batch_size, 0), device=self.device)\n",
    "        for t in range(sequence_length):\n",
    "            prob = self(x, prev_bits)\n",
    "            y_t = y[:, t:t+1]  # Ziel-Bit für den aktuellen Zeitschritt\n",
    "            #### Test\n",
    "            y_string = y[:, 0:t+1]\n",
    "            y_nn = y_string\n",
    "            if t == 0:\n",
    "                y_nn = prob\n",
    "            else:\n",
    "                y_nn = torch.cat((prev_bits, prob), dim=1)\n",
    "            ####\n",
    "                \n",
    "            #loss += loss_fn(prob, y_t)\n",
    "            loss += loss_fn(y_nn, y_string)\n",
    "            # Aktualisieren der bisher generierten Bits mit dem Ziel-Bit\n",
    "            prev_bits = torch.cat((prev_bits, y_t), dim=1)\n",
    "        log_loss(loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "\n",
    "'''\n",
    "max_iter = 400\n",
    "N = 100\n",
    "param = 2\n",
    "bits = 4\n",
    "x = torch.rand(N, param)\n",
    "y = torch.randint(0, 2, (N, bits), dtype=torch.float)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x, y), batch_size=N)\n",
    "    \n",
    "#return dataloader, input_size, hidden_size\n",
    "\n",
    "model = BitGeneratorModel(param)\n",
    "trainer = pl.Trainer(max_epochs=max_iter, accelerator='auto', devices='auto')\n",
    "trainer.fit(model, dataloader)\n",
    "'''\n",
    "\n",
    "cluster_string()\n",
    "\n",
    "QAOAenergie = 0\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from([0, 1, 2, 3])\n",
    "#G.add_nodes_from([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "#G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 0)])\n",
    "#G.add_nodes_from([0, 1, 2, 3, 4, 5])\n",
    "#G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4), (4, 5),(5,0)])\n",
    "#G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4), (4, 5),(5,6), (6, 7), (7,0)])\n",
    "G.add_edges_from([(0, 2), (1, 2), (1, 3), (2, 3)]) # Random Graph 1 - LSG: -3 und [0010], [0110], [0011]\n",
    "#G.add_edges_from([(0, 2), (1, 2), (1, 3)]) # Random Graph 2 - LSG: -3 und [0011]\n",
    "#G.add_edges_from([(0, 1), (0, 2), (1, 2), (2, 3)]) # Random Graph 3 - LSG: -3 und [0010], [0110], [0101]\n",
    "\n",
    "def generate_bitstrings(model, x, N, L):\n",
    "    prev_bits = torch.zeros((N, 0))\n",
    "    for t in range(L):\n",
    "        prob = model(x, prev_bits)\n",
    "        # Aktualisieren der bisher generierten Bits mit dem Ziel-Bit\n",
    "        prev_bits = torch.cat((prev_bits, prob), dim=1)\n",
    "        prev_bits = torch.bernoulli(prev_bits)\n",
    "    bits = prev_bits #torch.bernoulli(prev_bits)\n",
    "    #bits = (prev_bits>0.5).float()\n",
    "    return bits.detach().cpu().numpy()\n",
    "\n",
    "def create_data(betas, gammas, n=100, max_iter=1000, showDia=False):\n",
    "    if not isinstance(betas , list): raise ValueError('Parameter müssen Listen sein, keine Arrays')\n",
    "    parameters = betas + gammas\n",
    "    \n",
    "    global G\n",
    "    bitlength = G.number_of_nodes()\n",
    "    #input_size = 2\n",
    "    gw = nx.to_numpy_array(G)\n",
    "    res, shadow, expec= run_single_qaoa_cir(G, parameters)\n",
    "    global QAOAenergie\n",
    "    QAOAenergie = shadow.get_energy_list()[0]\n",
    "    shadow.eliminated_equivalent()\n",
    "    \n",
    "    print(\"QAOA Energie: \", QAOAenergie)\n",
    "    Y = shadow.shadowList[0].shadow\n",
    "    #Y = [[1,0,1,0] for _ in range(1000)]\n",
    "\n",
    "    meanY = np.mean(Y, axis=0)\n",
    "    print(\"Durchschnitt pro Bit (QAOA)\", meanY)\n",
    "    \n",
    "    X = [parameters for i in range(len(Y))]\n",
    "    N = len(Y)\n",
    "    x = torch.tensor(X, dtype=torch.float32)\n",
    "    #x = torch.zeros_like(x)\n",
    "    #x = torch.ones_like(x)\n",
    "    #x = torch.rand(N, len(parameters))\n",
    "    y = torch.tensor(Y, dtype=torch.float32)\n",
    "    \n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x, y), batch_size=N)\n",
    "    num_param = len(parameters)\n",
    "    model = BitGeneratorModel(num_param, hidden_size=4) # MyPixelRNN(num_param, hidden_size)\n",
    "    trainer = pl.Trainer(max_epochs=max_iter, accelerator='auto', devices='auto')\n",
    "    '''\n",
    "    tuner = pl.tuner.tuning.Tuner(trainer)\n",
    "    lr_finder = tuner.lr_find(model,\n",
    "                                    train_dataloaders=dataloader,\n",
    "                                    min_lr=0.000000001,\n",
    "                                    max_lr=1.0,\n",
    "                                    early_stop_threshold=None,\n",
    "                                    num_training=max_iter) # max epoche (maximal 1000)\n",
    "    fig = lr_finder.plot(suggest=True)\n",
    "    plt.title(\"LR Finder\")\n",
    "    fig.show()\n",
    "    model.hparams.lr = lr_finder.suggestion()\n",
    "    '''\n",
    "    trainer.fit(model, dataloader)\n",
    "\n",
    "    if(showDia):\n",
    "        plt.figure()\n",
    "        plt.title(\"Loss\")\n",
    "        plt.plot(loss_index, loss_curve)\n",
    "        plt.show()\n",
    "    array = generate_bitstrings(model, x, N, bitlength)\n",
    "    meanNN = np.mean(array, axis=0)\n",
    "    print(\"Durchschnitt pro Bit (NN)\", meanNN)\n",
    "    gw = nx.to_numpy_array(G)\n",
    "    \n",
    "    NNshadow = ShadowCollection([ShadowShot(array, parameters)], gw)\n",
    "    NNenergie = NNshadow.get_energy_list()[0]\n",
    "\n",
    "    print(\"Energie von NN\", NNenergie)\n",
    "\n",
    "    return NNenergie, QAOAenergie\n",
    "\n",
    "def create_data_QAOA(layers = 1, max_iter = 100, shots=1024, randomGraph=False, learning_sample_cap=None, split_mix=None, callByExp=None, saveBitsString=None):\n",
    "    global G\n",
    "    bitlength = G.number_of_nodes()\n",
    "    if randomGraph:\n",
    "        G = nx.gnp_random_graph(bitlength, 0.5)\n",
    "        for (u, v) in G.edges():\n",
    "            G.edges[u,v]['weight'] = 1\n",
    "    gw = nx.to_numpy_array(G)\n",
    "    res, shadow, expec= run_qaoa(G, layers=layers, shots=shots, callByExp=callByExp)\n",
    "    #global QAOAenergie\n",
    "    QAOAenergies = shadow.get_energy_list()\n",
    "    shadow.eliminated_equivalent()\n",
    "    NNenergies = []\n",
    "    param_list = []\n",
    "    for sh in shadow.shadowList:\n",
    "        parameters = sh.parameters\n",
    "        Y = sh.shadow\n",
    "        if learning_sample_cap!=None:\n",
    "            Y = Y[:learning_sample_cap]\n",
    "        X = [parameters for i in range(len(Y))]\n",
    "        N = len(Y)\n",
    "        x = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "        dataloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x, y), batch_size=N)\n",
    "        num_param = len(parameters)\n",
    "        model = BitGeneratorModel(num_param, hidden_size=4) # MyPixelRNN(num_param, hidden_size)\n",
    "        trainer = pl.Trainer(max_epochs=max_iter, accelerator='auto', devices='auto')\n",
    "        trainer.fit(model, dataloader)\n",
    "\n",
    "        array = generate_bitstrings(model, x, N, bitlength)\n",
    "        meanNN = np.mean(array, axis=0)\n",
    "        print(\"Durchschnitt pro Bit (NN)\", meanNN)\n",
    "        gw = nx.to_numpy_array(G)\n",
    "\n",
    "        if split_mix != None:\n",
    "            array = array[:split_mix[1]]\n",
    "            array = list(array)\n",
    "            array += Y[:split_mix[0]]\n",
    "        \n",
    "        NNshadow = ShadowCollection([ShadowShot(array, parameters)], gw)\n",
    "        NNenergie = NNshadow.get_energy_list()[0]\n",
    "\n",
    "        NNenergies.append(NNenergie)\n",
    "        param_list.append(parameters)\n",
    "    \n",
    "    return QAOAenergies, NNenergies, param_list\n",
    "\n",
    "def create_data_mit_abweichung_pool(konst_betas, konst_gammas, beta_grenzen, gamma_grenzen, random_sets=5, max_iter=600, showDia=False, savePath=None, rndMitAnaly=False):\n",
    "    X = []\n",
    "    Y = []\n",
    "    global G\n",
    "    bitlength = G.number_of_nodes()\n",
    "    for i in range(random_sets):\n",
    "        beta = random.uniform(beta_grenzen[0], beta_grenzen[1])\n",
    "        gamma = random.uniform(gamma_grenzen[0], gamma_grenzen[1])\n",
    "        if rndMitAnaly:\n",
    "            konst_betas.append(beta)\n",
    "            konst_gammas.append(gamma)\n",
    "            konst_betas.sort()\n",
    "            konst_gammas.sort()\n",
    "        parameters = [beta, gamma]\n",
    "        res, shadow, expec= run_single_qaoa_cir(G, parameters)\n",
    "        shadow.eliminated_equivalent()\n",
    "        Y_ = shadow.shadowList[0].shadow        \n",
    "        X_ = [parameters for i in range(len(Y_))]\n",
    "\n",
    "        X += X_\n",
    "        if not isinstance(Y_, list): Y_ = Y_.tolist()\n",
    "        Y += Y_\n",
    "    x = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(Y, dtype=torch.float32)\n",
    "    print(x.size(), y.size())\n",
    "    N = len(Y)\n",
    "    dataloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x, y), batch_size=N)\n",
    "    num_param = len(parameters)\n",
    "    model = BitGeneratorModel(num_param, hidden_size=4)\n",
    "    #model = BitGeneratorModel2(num_param, hidden_size=4)\n",
    "    trainer = pl.Trainer(max_epochs=max_iter, accelerator='auto', devices='auto')\n",
    "    trainer.fit(model, dataloader)\n",
    "\n",
    "    if(showDia):\n",
    "        plt.figure()\n",
    "        plt.title(\"Loss\")\n",
    "        plt.plot(loss_index, loss_curve)\n",
    "        plt.show()\n",
    "    para_list = []\n",
    "    QAOAenergies = []\n",
    "    NNenergies = []\n",
    "    fileString = \"\"\n",
    "    for beta, gamma in zip(konst_betas, konst_gammas):\n",
    "        konst_parameters = (beta, gamma)\n",
    "        res, shadow, expec= run_single_qaoa_cir(G, konst_parameters)\n",
    "        shadow.eliminated_equivalent()        \n",
    "        X_ = [parameters for i in range(len(Y))]\n",
    "        x_ = torch.tensor(X, dtype=torch.float32)\n",
    "        QAOAenergie = shadow.get_energy_list()[0]\n",
    "        array = generate_bitstrings(model, x_, N, bitlength)\n",
    "        gw = nx.to_numpy_array(G)\n",
    "    \n",
    "        NNshadow = ShadowCollection([ShadowShot(array, parameters)], gw)\n",
    "        NNenergie = NNshadow.get_energy_list()[0]\n",
    "\n",
    "        para_list.append(konst_parameters)\n",
    "        QAOAenergies.append(QAOAenergie)\n",
    "        NNenergies.append(NNenergie)\n",
    "\n",
    "        if not savePath==None:\n",
    "            fileString += str(QAOAenergie) +\"\\t\" + str(NNenergie) +\"\\t\"\n",
    "            for p in konst_parameters:\n",
    "                fileString += str(p) +\"\\t\"\n",
    "            fileString += \"\\n\"\n",
    "    if  not savePath==None:\n",
    "        f = open(savePath, \"w\")\n",
    "        f.write(fileString)\n",
    "        f.close()\n",
    "\n",
    "        \n",
    "    return para_list, QAOAenergies, NNenergies\n",
    "\n",
    "def create_data_mit_abweichung(betas, gammas, abweichung, n=100, max_iter=1000, showDia=False):\n",
    "    if not isinstance(betas , list): raise ValueError('Parameter müssen Listen sein, keine Arrays')\n",
    "    parameters = betas + gammas\n",
    "    para_minus = [i - abweichung for i in parameters]\n",
    "    para_plus = [i + abweichung for i in parameters]\n",
    "    \n",
    "    global G\n",
    "    bitlength = G.number_of_nodes()\n",
    "    #input_size = 2\n",
    "    gw = nx.to_numpy_array(G)\n",
    "    res, shadow, expec= run_single_qaoa_cir(G, parameters)\n",
    "    _, shadow_minus, _= run_single_qaoa_cir(G, para_minus)\n",
    "    _, shadow_plus, _= run_single_qaoa_cir(G, para_plus)\n",
    "    global QAOAenergie\n",
    "    QAOAenergie = shadow.get_energy_list()[0]\n",
    "    QAOAenergie_p = shadow_plus.get_energy_list()[0]\n",
    "    QAOAenergie_m = shadow_minus.get_energy_list()[0]\n",
    "    shadow.eliminated_equivalent()\n",
    "    \n",
    "    print(\"QAOA Energie: \", QAOAenergie, QAOAenergie_p, QAOAenergie_m)\n",
    "    print(\"QAOA Energie+: \", QAOAenergie_p)\n",
    "    print(\"QAOA Energie-: \", QAOAenergie_m)\n",
    "    Y = shadow.shadowList[0].shadow\n",
    "    #Y = [[1,0,1,0] for _ in range(1000)]\n",
    "\n",
    "    meanY = np.mean(Y, axis=0)\n",
    "    print(\"Durchschnitt pro Bit (QAOA)\", meanY)\n",
    "    \n",
    "    X = [parameters for i in range(len(Y))]\n",
    "    X_minus = [para_minus for i in range(len(Y))]\n",
    "    X_plus = [para_plus for i in range(len(Y))]\n",
    "    N = len(Y)\n",
    "    x = torch.tensor(X, dtype=torch.float32)\n",
    "    x_plus = torch.tensor(X_plus, dtype=torch.float32)\n",
    "    x_minus = torch.tensor(X_minus, dtype=torch.float32)\n",
    "    \n",
    "    y = torch.tensor(Y, dtype=torch.float32)\n",
    "    \n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x, y), batch_size=N)\n",
    "    num_param = len(parameters)\n",
    "    model = BitGeneratorModel(num_param, hidden_size=4) # MyPixelRNN(num_param, hidden_size)\n",
    "    trainer = pl.Trainer(max_epochs=max_iter, accelerator='auto', devices='auto')\n",
    "    \n",
    "    trainer.fit(model, dataloader)\n",
    "\n",
    "    array = generate_bitstrings(model, x, N, bitlength)\n",
    "    array_plus = generate_bitstrings(model, x_plus, N, bitlength)\n",
    "    array_minus = generate_bitstrings(model, x_minus, N, bitlength)\n",
    "\n",
    "    \n",
    "    gw = nx.to_numpy_array(G)\n",
    "    \n",
    "    NNshadow = ShadowCollection([ShadowShot(array, parameters)], gw)\n",
    "    NNshadow_plus = ShadowCollection([ShadowShot(array_plus, para_plus)], gw)\n",
    "    NNshadow_minus = ShadowCollection([ShadowShot(array_minus, para_minus)], gw)\n",
    "    NNenergie = NNshadow.get_energy_list()[0]\n",
    "    NNenergie_p = NNshadow_plus.get_energy_list()[0]\n",
    "    NNenergie_m = NNshadow_minus.get_energy_list()[0]\n",
    "\n",
    "    print(\"Energie von NN\", NNenergie)\n",
    "    print(\"Energie von NN+\",NNenergie_p)\n",
    "    print(\"Energie von NN-\", NNenergie_m)\n",
    "\n",
    "    return NNenergie, QAOAenergie\n",
    "\n",
    "problem_gamma = (3 * np.pi / 4) * 0.75 + np.pi * 0.25\n",
    "\n",
    "# Beispiel mit Graph\n",
    "#create_data([2], [2], max_iter=400, showDia=True)\n",
    "#create_data_mit_abweichung([2], [2], abweichung=0.05, max_iter=400, showDia=True)\n",
    "#create_data_QAOA()\n",
    "\n",
    "# gibt File aus mit konstanten beta und gamma variert\n",
    "def writefile(n=1, epoch=400):\n",
    "    #logging.getLogger('lightning').setLevel(0)\n",
    "    fileString = \"\"\n",
    "    gammas = np.linspace(0, np.pi, n)\n",
    "    i = 1\n",
    "    for gamma in gammas:\n",
    "        print(\"Beginn von Nummer: \", i)\n",
    "        i += 1\n",
    "        NNenergie, QAOAenergie = create_data([2], [gamma], max_iter=epoch, showDia=False)\n",
    "        fileString += str(QAOAenergie) +\"\\t\" + str(NNenergie) +\"\\t\" + str(gamma) + \"\\n\"\n",
    "    file_name = \"LSTM beta 2 hidden 4 bits 4 Random Graph3_2.txt\"\n",
    "    file_name = cluster_string(file_name)\n",
    "    f = open(file_name, \"w\")\n",
    "    f.write(fileString)\n",
    "    f.close()\n",
    "    print(\"fertig!!\")\n",
    "\n",
    "# gibt FIle aus für QAOA\n",
    "def writefile_QAOA(n=1, epoch=400, layers=1, shots=1024, learning_sample_cap=None, split_mix=None, saveBitsString=False):\n",
    "    fileString = \"\"\n",
    "    QAOAenergies, NNenergies, param_list = create_data_QAOA(max_iter=epoch, layers=layers, shots=shots, learning_sample_cap=learning_sample_cap, split_mix= split_mix)\n",
    "    i = 1\n",
    "    for q,n,ps in zip(QAOAenergies, NNenergies, param_list):\n",
    "        print(\"Beginn von Nummer: \", i)\n",
    "        i += 1\n",
    "        fileString += str(q) +\"\\t\" + str(n) +\"\\t\"\n",
    "        for p in ps:\n",
    "            fileString += str(p) +\"\\t\"\n",
    "        fileString += \"\\n\"\n",
    "    file_name = \"LSTM beta QAOA hidden 4 bits 8 layer1_5.txt\"\n",
    "    file_name = cluster_string(file_name)\n",
    "    f = open(file_name, \"w\")\n",
    "    f.write(fileString)\n",
    "    f.close()\n",
    "    print(\"fertig!!\")\n",
    "\n",
    "def writefile_QAOA_training_plus_NN(n=1, epoch=600, layers=1, shots=1024, add_NN_samples=None, max_iter=None, fileprefix=\"\"):\n",
    "    if add_NN_samples==None: add_NN_samples=shots\n",
    "    def unterfunktion(expec, G, theta, result, shadow, running_index):\n",
    "        #print(running_index)\n",
    "        \n",
    "        bitlength = G.number_of_nodes()\n",
    "        Y = shadow\n",
    "        X = [theta for i in range(len(Y))]\n",
    "        N = len(Y)\n",
    "        x = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(Y, dtype=torch.float32)\n",
    "        max_iter=epoch\n",
    "\n",
    "        dataloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x, y), batch_size=N)\n",
    "        num_param = len(theta)\n",
    "        model = BitGeneratorModel(num_param, hidden_size=4) # MyPixelRNN(num_param, hidden_size)\n",
    "        trainer = pl.Trainer(max_epochs=max_iter, accelerator='auto', devices='auto')\n",
    "        trainer.fit(model, dataloader)\n",
    "\n",
    "        gw = nx.to_numpy_array(G)\n",
    "        \n",
    "        array = generate_bitstrings(model, x, N, bitlength)\n",
    "        shadow += list(array[:add_NN_samples])\n",
    "        NNshadow = ShadowCollection([ShadowShot(shadow, theta)], gw)\n",
    "        NNexpec = NNshadow.get_energy_list()[0]\n",
    "\n",
    "\n",
    "        return NNexpec, shadow\n",
    "\n",
    "    fileString = \"\"\n",
    "    res1, shadow1, expec1 = run_qaoa(G, layers=layers, shots=shots, callByExp=unterfunktion, max_iter=max_iter) # QAOA Simualtion gemischt mit NN\n",
    "    res2, shadow2, expec2 = run_qaoa(G, layers=layers, shots=shots, seed=None, max_iter=max_iter) # QAOA Simualtion ohne NN\n",
    "    res3, shadow3, expec3 = run_qaoa(G, layers=layers, shots=shots + add_NN_samples, max_iter=max_iter) # QAOA Simualtion ohne NN, aber mit extra Samples\n",
    "    i = 1\n",
    "    for s1, s2, s3 in zip(shadow1.get_energy_list(), shadow2.get_energy_list(), shadow3.get_energy_list()):\n",
    "        fileString += str(s1) +\"\\t\" + str(s2) +\"\\t\" + str(s3)\n",
    "        fileString += \"\\n\"\n",
    "    file_name = \"LSTM Mixture QAOA hidden 4 random graph1 layer1_1.txt\"\n",
    "    file_name = fileprefix + file_name\n",
    "    file_name = cluster_string(file_name)\n",
    "    f = open(file_name, \"w\")\n",
    "    f.write(fileString)\n",
    "    f.close()\n",
    "    print(\"fertig!!\")\n",
    "\n",
    "#create_data_mit_abweichung_pool([0.5, 1, 1.5, 2, 2.5, 3, 3.5], [0.5, 1, 1.5, 2, 2.5, 3, 3.5], beta_grenzen=[1.5,2.5], gamma_grenzen=[1.5,2.5], showDia=True, savePath=\"LSTM Model2 parameter random pool_1\", random_sets=10, max_iter=300, rndMitAnaly=True)\n",
    "#writefile_QAOA(epoch=400, layers=1, shots=1024)\n",
    "#writefile_QAOA_training_plus_NN(epoch=600, layers=4, shots=960, add_NN_samples=64, max_iter=80, fileprefix=\"896vs64\")\n",
    "#writefile_QAOA_training_plus_NN(epoch=600, layers=4, shots=896, add_NN_samples=128, max_iter=80, fileprefix=\"896vs128\")\n",
    "#writefile_QAOA_training_plus_NN(epoch=600, layers=4, shots=768, add_NN_samples=256, max_iter=80, fileprefix=\"768vs256\")\n",
    "#writefile_QAOA_training_plus_NN(epoch=600, layers=4, shots=512, add_NN_samples=512, max_iter=80, fileprefix=\"512vs512\")\n",
    "#writefile_QAOA_training_plus_NN(epoch=600, layers=4, shots=256, add_NN_samples=768, max_iter=80, fileprefix=\"256vs768\")\n",
    "#writefile_QAOA_training_plus_NN(epoch=600, layers=4, shots=128, add_NN_samples=896, max_iter=80, fileprefix=\"128vs896\")\n",
    "#writefile_QAOA_training_plus_NN(epoch=600, layers=4, shots=64, add_NN_samples=980, max_iter=80, fileprefix=\"64vs980\")\n",
    "#writefile_QAOA_training_plus_NN(epoch=600, layers=4, shots=32, add_NN_samples=992, max_iter=80, fileprefix=\"32vs992\")\n",
    "writefile_QAOA_training_plus_NN(epoch=600, layers=4, shots=16, add_NN_samples=1008, max_iter=80, fileprefix=\"16vs1008\")\n",
    "#writefile_QAOA_training_plus_NN(epoch=600, layers=4, shots=8, add_NN_samples=1016, max_iter=80, fileprefix=\"8vs1016\")\n",
    "\n",
    "#writefile(n = 100, epoch=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qiskit2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
